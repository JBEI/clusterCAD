{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pprint import pprint\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem as chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "helper = imp.load_source('helper', './pks/helper.py')\n",
    "domain = imp.load_source('domain', './pks/domain.py')\n",
    "pks = imp.load_source('pks', './pks/pks.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_path = './mibig'\n",
    "file_names = glob.glob(os.path.join(file_path, '*.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# MiBiG Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Erythromycin\n",
    "test_file = './mibig/BGC0000055.json'\n",
    "with open(test_file) as json_file:\n",
    "    test_data = json.load(json_file)\n",
    "#pprint(test_data.keys())\n",
    "#pprint(test_data['general_params'].keys())\n",
    "#pprint(test_data['general_params'])\n",
    "pprint(test_data['general_params']['loci']['nucl_acc'][0]['Accession'])\n",
    "pprint(test_data['general_params']['Polyketide'])\n",
    "#pprint([compound['compound'] for compound in test_data['general_params']['compounds']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# See what kind of PKS subtype labels there are\n",
    "labels = []\n",
    "for file_name in file_names:\n",
    "    with open(file_name) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    try:\n",
    "        labels.extend(json_data['general_params']['Polyketide']['pks_subclass'])\n",
    "    except KeyError:\n",
    "        pass\n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This gets all the modular type I PKSs contained in MiBiG (at least the examples that are so labeled)\n",
    "t1pks = []\n",
    "for file_name in file_names:\n",
    "    with open(file_name) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    try:        \n",
    "        if len(set(['Modular type I', 'Modular Type I', 'Type I']).intersection(set(json_data['general_params']['Polyketide']['pks_subclass']))) > 0:\n",
    "            accession = json_data['general_params']['loci']['nucl_acc'][0]['Accession']\n",
    "            t1pks.append((file_name.split('/')[-1].split('.')[0], accession))\n",
    "    except KeyError:\n",
    "        pass\n",
    "#print(len(t1pks))\n",
    "#print([entry[1] for entry in t1pks])\n",
    "# with open('./mibig/antismash/t1pks_list.txt', 'w') as f:\n",
    "#     for entry in t1pks:\n",
    "#         f.write(entry[0].split('/')[-1].split('.')[0] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of clusters found\n",
    "print('%d potential type I modular PKS clusters found!' %(len(t1pks)))\n",
    "# Iterate over list of type I modular PKSs\n",
    "for i in range(len(t1pks)):\n",
    "    entry = t1pks[i]\n",
    "    # This prints the product compounds for the clusters\n",
    "    with open(os.path.join(file_path, entry[0] + '.json')) as json_file:\n",
    "        mibig_data = json.load(json_file)\n",
    "        pprint([compound['compound'] for compound in mibig_data['general_params']['compounds']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get keys for genes\n",
    "keys = []\n",
    "for i in range(len(t1pks)):\n",
    "    entry = t1pks[i]\n",
    "    # This prints the product compounds for the clusters\n",
    "    with open(os.path.join(file_path, entry[0] + '.json')) as json_file:\n",
    "        mibig_data = json.load(json_file)\n",
    "        # Iterate over all subunits getting domains\n",
    "        keys.extend(list(mibig_data['general_params']['Polyketide'].keys()))\n",
    "print(set(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get set of catalytic domains found across all type I modular PKSs\n",
    "domains = []\n",
    "lin_cyc = []\n",
    "for i in range(len(t1pks)):\n",
    "    entry = t1pks[i]\n",
    "    # This prints the product compounds for the clusters\n",
    "    with open(os.path.join(file_path, entry[0] + '.json')) as json_file:\n",
    "        mibig_data = json.load(json_file)\n",
    "        # Iterate over all subunits getting domains\n",
    "        try:\n",
    "            pks_genes = mibig_data['general_params']['Polyketide']['mod_pks_genes']\n",
    "            lin_cyc.append(mibig_data['general_params']['Polyketide']['lin_cycl_pk'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for subunit in pks_genes:\n",
    "            subunit_name = re.sub(r'\\s+', '', subunit['mod_pks_gene'])\n",
    "            subunit_modules = subunit['pks_module']\n",
    "            for module in subunit_modules:\n",
    "                domains.extend(module['pks_domains'])\n",
    "print(set(domains))\n",
    "print(set(lin_cyc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# antiSMASH output from Tyler\n",
    "antismash_file_path = './mibig/antismash/split_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Proccessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_subunit_modules(sec_met): \n",
    "    '''This function takes as input the the list recorded by feature.qualifiers['sec_met'] for a module in a PKS\n",
    "       cluster. This assumes that feature.type=='CDS' and that feature.qualifiers has the key 'sec_met'.\n",
    "       The function returns a dict corresponding to the modules in the subunit, indexed starting from zero within\n",
    "       the subunit. If the first entry of 'sec_met' is not 'Type: t1pks' then nothing is returned.\n",
    "    '''\n",
    "    # Initialize dict for the subunit\n",
    "    # keys: module number\n",
    "    # values: OrderedDict of domains in module\n",
    "    #         within OrderedDict, key is domain name and value is a length 2 list where the\n",
    "    #         first element is a dictionary {start:, stop:} and the second element is specificity dictionary \n",
    "    subunit = {}\n",
    "    \n",
    "    # This is for the current module (function processes subunit which may have more than one module)\n",
    "    module_index = 0  # key for module\n",
    "    module_domains = [] # list of domains in module\n",
    "    old_module_domains = [] # pre-initialize in case subunit starts with a domain\n",
    "                            # that is expected to end the module\n",
    "    \n",
    "    # This is how domains appear in sec_met:\n",
    "    # ['PKS_AT', 'PKS_KS', 'PKS_KR', 'PKS_DH', 'PKS_ER', 'ACP', 'Thioesterase']\n",
    "    # Iterate over the entries in sec_met, and add them to the module_domains list \n",
    "    for entry in sec_met:    \n",
    "        # Split entry into a list\n",
    "        entrysplit = [item.strip() for item in entry.split(';') if item != '']\n",
    "        # Split part of entry that is expected to describe catalytic domain\n",
    "        domainsplit = entrysplit[0].split()\n",
    "        # This is just different ways of processing the name of the domain depending\n",
    "        # on how the name of the domain is formatted\n",
    "        if ' '.join(domainsplit[:2]) == 'NRPS/PKS Domain:' and len(domainsplit) > 2:\n",
    "            # Note that we want to make sure that there is a leading 'PKS_' before we do our trimming\n",
    "            if domainsplit[2].split('_')[0] == 'PKS':\n",
    "                if domainsplit[2] in ['PKS_Docking_Nterm', 'PKS_Docking_Cterm']:\n",
    "                    domaintype = domainsplit[2]\n",
    "                else:\n",
    "                    # We trim off the leading 'PKS_'\n",
    "                    # Assume 'DH2' and 'DHt' are the same as 'DH' \n",
    "                    domaintype = domainsplit[2].split('_')[-1].replace('DHt', 'DH').replace('DH2', 'DH')\n",
    "            # Special case of 'CAL' domain\n",
    "            elif domainsplit[2] == 'CAL_domain':\n",
    "                domaintype = 'CAL'\n",
    "            else:\n",
    "                domaintype = domainsplit[2]\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        # DEBUG\n",
    "#        print(domaintype)\n",
    "    \n",
    "        # These are the catlytic domains that we want to recognize\n",
    "        if domaintype not in ['KS', 'AT', 'KR', 'DH', 'ER', 'ACP', 'Thioesterase', \n",
    "                              'cMT', 'oMT', 'CAL', 'PCP', \n",
    "                              'Heterocyclization', 'AMP-binding', \n",
    "                              'Condensation_DCL', 'Condensation_LCL',\n",
    "                              'PKS_Docking_Nterm', 'PKS_Docking_Cterm']:\n",
    "            # Break out of for loop and stop looking for additional catalytic domains if \n",
    "            # we encountered a domain that we don't recognize\n",
    "            # we end up excluding any subunit that has a non-recognized catalytic domain\n",
    "            # this is dealt with by checking subunits against those that are expected to be recognized\n",
    "            # as determined by the MiBiG JSON file\n",
    "            break    \n",
    "        # Get the obundaries of the catalytic domain\n",
    "        boundaries = [int(bound) for bound in domainsplit[3].replace('(', '').replace(')', '').replace('.', '').split('-')]\n",
    "        \n",
    "        # Here, we add each domain to a list, which will be converted to an OrderedDict\n",
    "        # based on whether or not the domain is expected to have substrate specificity annotations\n",
    "        if domaintype in ['KS', 'DH', 'ER', 'ACP', 'cMT', 'oMT', 'CAL', 'PCP',\n",
    "                          'Heterocylization', 'AMP-binding', \n",
    "                          'Condensation_DCL', 'Condensation_LCL',\n",
    "                          'PKS_Docking_Nterm', 'PKS_Docking_Cterm']:   # Recall that we trimmed leading 'PKS_'\n",
    "            module_domains.append((domaintype, [{'start': boundaries[0], 'stop': boundaries[1]}]))\n",
    "        # Include substrate and stereospecificity annotations for AT and KR domains respectively\n",
    "        if domaintype in ['AT', 'KR']:   # Recall that we trimmed leading 'PKS_'\n",
    "            notesdict = {}\n",
    "            for note in entrysplit[1:]:\n",
    "                item = note.split(': ')\n",
    "                notesdict[item[0]] = item[1]\n",
    "            module_domains.append((domaintype, [{'start': boundaries[0], 'stop': boundaries[1]}, notesdict]))\n",
    "                \n",
    "        # End of the module has been reached of the domain is 'ACP' or 'PCP\n",
    "        if domaintype in ['ACP', 'PCP']:\n",
    "            domains_present = [d[0] for d in module_domains]\n",
    "            # Make sure every module has an AT, or else it isn't a valid module and we just ignore it\n",
    "            # This means it will be excluded from the subunit, which makes sense since we can't \n",
    "            # really perform a polyketide chain extension without an AT\n",
    "            if 'AT' in domains_present:            \n",
    "                subunit[module_index] = OrderedDict(module_domains)\n",
    "                old_module_domains = module_domains\n",
    "                module_index += 1\n",
    "            else:\n",
    "                old_module_domains = []\n",
    "            module_domains = []\n",
    "        # These domains may come after the ACP or PCP, so if they are encountered, we add\n",
    "        # them to previous module and keep going forward\n",
    "        if domaintype in ['Thioesterase', 'PKS_Docking_Cterm', 'Condensation_LCL']:\n",
    "            # Overwrite previous subunit, or else will have duplicate entries\n",
    "            old_module_domains.append((domaintype, [{'start': boundaries[0], 'stop': boundaries[1]}]))\n",
    "            subunit[module_index - 1] = OrderedDict(old_module_domains)\n",
    "            module_domains = []\n",
    "            \n",
    "    return subunit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_gene_data(record):\n",
    "    '''Takes as input a record read in from an MiBiG GenBank file using SeqIO.read() and outputs PKS data \n",
    "       from that record. Data will be comprised of PKS subunits and standalone PKS genes.\n",
    "    '''\n",
    "    # Get list to hold information about all genes that are in the record\n",
    "    gene_data = []\n",
    "    \n",
    "    # Only the \"CDS\" features are potentially genes\n",
    "    # Here we get genes that aren't necessarily subunits\n",
    "    for feature in record.features:\n",
    "        # These are the features we are interested in\n",
    "        if feature.type == 'CDS' and 'protein_id' in feature.qualifiers.keys() and 'gene' in feature.qualifiers.keys(): \n",
    "            # This gets the location of the feature\n",
    "            location = feature.location\n",
    "            # Potential information about gene\n",
    "            if 'product' in feature.qualifiers.keys():\n",
    "                description = feature.qualifiers['product'][0]\n",
    "            gene_data.append([feature.qualifiers['protein_id'][0],\n",
    "                              feature.qualifiers['gene'][0],\n",
    "                             ])\n",
    "            # Feature may not be a PKS module and therefore may not have have subunits \n",
    "            # (this will be overwritten if it does have subunits)\n",
    "            subunit_modules = None\n",
    "            # Information if gene is PKS subunit\n",
    "            if 'sec_met' in feature.qualifiers.keys() and len(feature.qualifiers['sec_met']) > 3:\n",
    "                if feature.qualifiers['sec_met'][3] in ['NRPS/PKS subtype: Type I Modular PKS', \n",
    "                                                        'NRPS/PKS subtype: PKS-like protein',\n",
    "                                                        'NRPS/PKS subtype: PKS/NRPS-like protein',\n",
    "                                                        'NRPS/PKS subtype: Hybrid PKS-NRPS']:                    \n",
    "                    # DEBUG\n",
    "#                    print(feature.qualifiers['gene'][0])\n",
    "                    # This gets the subunit information                    \n",
    "                    subunit_modules = process_subunit_modules(feature.qualifiers['sec_met'])\n",
    "#                else:\n",
    "#                    print(feature)\n",
    "            \n",
    "            # More general information\n",
    "            gene_data[-1].extend([description, [location.start.position, location.end.position]])\n",
    "\n",
    "            # Subunit information (if it doesn't have subunit information, assumed to be a standalone enzyme)\n",
    "            if subunit_modules:\n",
    "                gene_data[-1].append(subunit_modules)\n",
    "\n",
    "            # General information about gene\n",
    "            gene_data[-1].append(feature.qualifiers['translation'][0])\n",
    "\n",
    "    return gene_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_json_module_validity(module_list):\n",
    "    '''Function that makes sure module specified in JSON file is valid,\n",
    "       that is to say, make sure that it contains KS, AT, and ACP or PCP.\n",
    "       AT least as of February 24, 2017, the names of these domains appear\n",
    "       only in the following forms in clusters that are annotated as Type I modular PKSs\n",
    "       ['KS', 'AT', 'T']\n",
    "       ['Ketosynthase', 'Acyltransferase', 'Thiolation (ACP/PCP)']\n",
    "    '''\n",
    "    at_check = len(set(['AT', 'Acyltransferase']).intersection(set(module_list)))\n",
    "    acp_check = len(set(['ACP', 'PCP', 'T', 'Thiolation (ACP/PCP)']).intersection(set(module_list)))\n",
    "\n",
    "    if at_check and acp_check:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def process_cluster(record, mibig_json):\n",
    "    '''Takes in a record and corresponding MiBiG json file\n",
    "       then returns pks.Cluster object representing the cluster.\n",
    "    '''\n",
    "    # Get information about the gene\n",
    "    gene_data = get_gene_data(record)\n",
    "    if len(gene_data) == 0:\n",
    "        return\n",
    "\n",
    "    # Initalize lists for subunits and standalones\n",
    "    # We make two dictionaries because sometimes the subunit name in the MiBiG JSON files\n",
    "    # is the gene name, e.g. eryA1, and sometimes it is the accession number, e.g. A0000000\n",
    "    unordered_subunits = {}\n",
    "    unordered_subunits_alt = {}\n",
    "    standalones = []\n",
    "     \n",
    "    # Recall that each entry in gene_data is a list\n",
    "    # [protein id, gene, product, [location start, location end], subunit dict (optional), translation]\n",
    "    \n",
    "    #####################\n",
    "    # Basic information #\n",
    "    #####################\n",
    "    \n",
    "    counter = 1\n",
    "    for gene in gene_data:\n",
    "        geneid = gene[0].strip()\n",
    "        genename = gene[1].strip()\n",
    "        genedesc = gene[2].strip()\n",
    "        genestart = gene[3][0]\n",
    "        genestop = gene[3][1]\n",
    "        genetranslation = gene[-1].strip()\n",
    "\n",
    "        # Just use length of gene_data to differentiate between standalones and subunits\n",
    "        if len(gene) == 6:\n",
    "            # We do this to take care of duplicated gene names, as is the case wity tylactone (BGC0000166)\n",
    "            if genename in unordered_subunits_alt.keys():\n",
    "                genename = genename + '_' + str(counter)\n",
    "                counter += 1\n",
    " \n",
    "            # Get subunit data from gene\n",
    "            genesubunitdata = gene[-2]\n",
    "            # Here we use the two dictionary options to save the unordered subunits\n",
    "            # Sometimes MiBiG uses geneid and sometimes it uses genename to reference subunits\n",
    "            unordered_subunits[geneid] = (genename, genedesc, genestart, genestop,\n",
    "                                            genesubunitdata, genetranslation)\n",
    "            unordered_subunits_alt[genename] = (geneid, genedesc, genestart, genestop,\n",
    "                                                genesubunitdata, genetranslation)\n",
    "        else:\n",
    "            # Standalones lack subunit and orphan entries\n",
    "            assert len(gene) == 5, gene\n",
    "            standalones.append(pks.Standalone(geneid, genename, genedesc, \n",
    "                                              genestart, genestop, genetranslation))\n",
    "        \n",
    "    #########################################\n",
    "    # JSON file has cyclization information #\n",
    "    #########################################\n",
    "\n",
    "    # Get ordered version of subunits from corresponding JSON file\n",
    "    with open(mibig_json) as json_file:\n",
    "        mibig_data = json.load(json_file)\n",
    "    \n",
    "    # Get PKS cyclization information\n",
    "    # this will be either 'Cyclic' or 'Linear'\n",
    "    try:\n",
    "        lin_cycl_pk = mibig_data['general_params']['Polyketide']['lin_cycl_pk']\n",
    "        if lin_cycl_pk == 'Cyclic':\n",
    "            cyclize = 'Cyclic'\n",
    "        elif lin_cycl_pk == 'Linear':\n",
    "            cyclize = 'Linear'\n",
    "        else:\n",
    "            raise Exception(\"lin_cycl_pk expected to be 'Cyclic' or 'Linear'.\")\n",
    "    except KeyError:\n",
    "        cyclize = 'Linear'\n",
    "            \n",
    "    #####################################\n",
    "    # JSON file has subunit information #\n",
    "    #####################################\n",
    "        \n",
    "    # Note that all gene data has now been processed, want to reprocess to get right ordering \n",
    "    # We strip out subunits that have invalid modules\n",
    "    try:\n",
    "        ordered_subunits = []\n",
    "        for subunit in mibig_data['general_params']['Polyketide']['mod_pks_genes']:\n",
    "            subunit_name = re.sub(r'\\s+', '', subunit['mod_pks_gene'])\n",
    "            subunit_modules = subunit['pks_module']\n",
    "\n",
    "            valid_subunit = True\n",
    "            # This checks if the module is valid\n",
    "            for module in subunit_modules:\n",
    "                # Just for debugging\n",
    "    #            print(module['pks_domains'])\n",
    "                if not check_json_module_validity(module['pks_domains']):\n",
    "                    valid_subunit = False\n",
    "            if valid_subunit:\n",
    "                ordered_subunits.extend(subunit_name.split(','))\n",
    "            else:\n",
    "                # Loop is broken once first invalid subunit is encountered\n",
    "                break\n",
    "        # If no valid subunits, then just return\n",
    "        if len(ordered_subunits) == 0:\n",
    "            print('\\tNo valid subunits!')\n",
    "            for subunit in mibig_data['general_params']['Polyketide']['mod_pks_genes']:\n",
    "                subunit_name = re.sub(r'\\s+', '', subunit['mod_pks_gene'])\n",
    "                subunit_modules = subunit['pks_module']\n",
    "                for module in subunit_modules:\n",
    "                    print(module['pks_domains'])\n",
    "            return\n",
    "        # This makes sure the subunit accession naming is consistent\n",
    "        # The purpose of these two 'if' statements is because there may be cases in the MiBiG JSON file\n",
    "        # where the name of the gene is for example, 'eryA1, A000000' and we want to keep consistant naming\n",
    "        if len(ordered_subunits[0]) >= 8:\n",
    "            ordered_subunits = [entry for entry in ordered_subunits if len(entry) >= 8]\n",
    "        if len(ordered_subunits) > 1:\n",
    "            if len(ordered_subunits[1]) >= 8:\n",
    "                ordered_subunits = [entry for entry in ordered_subunits if len(entry) >= 8]\n",
    "        # This is because sometimes the accession number under which the gene is recorded sometimes\n",
    "        # has a version number, and sometimes does not\n",
    "        if len(ordered_subunits[0].split('.')) == 1 and len(ordered_subunits[0]) == 8:\n",
    "            ordered_subunits = [entry + '.1' for entry in ordered_subunits]\n",
    "\n",
    "        # Check if subunit is in either dictionary\n",
    "        for isubunit,subunit in enumerate(ordered_subunits):\n",
    "            if subunit not in set(list(unordered_subunits.keys()) + list(unordered_subunits_alt.keys())):\n",
    "                print('Missing subunit: \"%s\"' %(subunit))\n",
    "                for gene in mibig_data['general_params']['Polyketide']['mod_pks_genes']:\n",
    "                    if gene['mod_pks_gene'] == subunit:\n",
    "                        module = gene['pks_module']\n",
    "                        for entry in module:\n",
    "                            print(entry['pks_domains'])\n",
    "                print(unordered_subunits.keys())\n",
    "                return\n",
    "    #    print([gene_ref for gene_ref in mibig_data['general_params']['Polyketide']['mod_pks_genes']])\n",
    "\n",
    "        # Determine whether to use standard or alternative dict\n",
    "        if len(ordered_subunits[0]) >= 8:\n",
    "            alt = False\n",
    "        else:\n",
    "            alt = True\n",
    "    \n",
    "    # Just use unordered gene order if the gene ordering is not already in the JSON file \n",
    "    except Exception:\n",
    "        ordered_subunits = list(unordered_subunits_alt.keys())\n",
    "        ordered_subunits.sort()\n",
    "        alt = True\n",
    "\n",
    "    ####################################\n",
    "    # This does the subunit reordering #\n",
    "    ####################################\n",
    "    # Initialize final list of subunits\n",
    "    subunits = []\n",
    "    \n",
    "    modules_seen = 0\n",
    "    for subunit in ordered_subunits:\n",
    "        # subunit data has form (id, description, start, stop, module dict, sequence)\n",
    "        if not alt:\n",
    "            subunitdata = unordered_subunits[subunit]\n",
    "        else:\n",
    "            subunitdata = unordered_subunits_alt[subunit]\n",
    "        # Initialize list to hold processed modules\n",
    "        # this is a list of module objects that we will pass to pks.Subunit()\n",
    "        modules = []\n",
    "        # This is the modules for the subunit\n",
    "        moduledata = subunitdata[-2]\n",
    "        \n",
    "        # We do this so we can lump in the loading didomain and TE on the first and last modules respectively\n",
    "        modulekeys = list(moduledata.keys())\n",
    "        imodule = 0\n",
    "        while imodule < len(modulekeys):\n",
    "            # Get info\n",
    "            keys = list(moduledata[modulekeys[imodule]].keys())\n",
    "            values = moduledata[modulekeys[imodule]].values()\n",
    "            # Process info according to loading or not\n",
    "            if modules_seen == 0:\n",
    "                loading = True                \n",
    "                # Don't name KSQ and ATL separate after all\n",
    "#                moduledict =  OrderedDict([(k.replace('KS', 'KS').replace('AT', 'AT'), v) \\\n",
    "#                                          if k in ['KS','AT'] \\\n",
    "#                                          else (k,v) \\\n",
    "#                                          for k,v in zip(keys,values)])\n",
    "                moduledict =  OrderedDict([(k,v) for k,v in zip(keys,values)])\n",
    "            else: \n",
    "                loading = False\n",
    "                moduledict = OrderedDict([(k,v) for k,v in zip(keys,values)])\n",
    "            # Determine whether module is terminal or not\n",
    "            if 'Thioesterase' in list(moduledata[modulekeys[imodule]].keys()):\n",
    "                terminal = cyclize\n",
    "            else:\n",
    "                terminal = None\n",
    "            imodule += 1\n",
    "            modules_seen += 1\n",
    "            try:\n",
    "                # This is to make sure we don't add subunits with invalid modules\n",
    "                # The check for errors here is to compare agains the predicted chemcial structure\n",
    "                domains_present = moduledict.keys()\n",
    "                if 'ACP' in domains_present or 'PCP' in domains_present:\n",
    "                    if 'AT' in domains_present or 'ATL' in domains_present:\n",
    "                        modules.append(pks.Module(moduledict, loading=loading, terminal=terminal))\n",
    "            except AssertionError as e:\n",
    "                print(moduledict)\n",
    "                print(type(e).__name__, e.args, subunit + ' ' + subunitdata[1])\n",
    "                raise Exception(type(e).__name__, e.args, subunit + ' ' + subunitdata[1])\n",
    "                break\n",
    "        # Add subunit to list\n",
    "        if len(modules) > 0:\n",
    "            if not alt:\n",
    "                subunits.append(pks.Subunit(subunit, subunitdata[0], subunitdata[1],\n",
    "                                            subunitdata[2], subunitdata[3], subunitdata[-1],\n",
    "                                            modules))\n",
    "            else:\n",
    "                subunits.append(pks.Subunit(subunitdata[0], subunit, subunitdata[1],\n",
    "                                            subunitdata[2], subunitdata[3], subunitdata[-1],\n",
    "                                            modules))\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        # We take the last Subunit object and change the TE contained in the subunit to cyclize if necessary\n",
    "        if cyclize:\n",
    "            final_module = subunits[-1].modules[-1]\n",
    "            if final_module.terminal == True:\n",
    "                assert 'Thioesterase' in list(final_module.domains.keys()), \\\n",
    "                    \"Terminal module lacks 'Thioesterase' in domains dictionary.\"\n",
    "                TE = final_module.operations[-1]\n",
    "                TE.cyclize = True\n",
    "        \n",
    "    return (subunits, standalones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Testing specific entries\n",
    "entry = ('BGC0000166', 'U78289')  # tylactone\n",
    "#entry = ('BGC0000033', 'AF497482') # calicheamicin\n",
    "\n",
    "with open(os.path.join(file_path, entry[0] + '.json')) as json_file:\n",
    "    mibig_data = json.load(json_file)\n",
    "#pprint(test_data.keys())\n",
    "#pprint(test_data['general_params'].keys())\n",
    "#pprint(test_data['general_params'])\n",
    "#pprint([gene_ref['mod_pks_gene'] for gene_ref in test_data['general_params']['Polyketide']['mod_pks_genes']])\n",
    "record = SeqIO.read(os.path.join(antismash_file_path, entry[0] + '.embl'), \"embl\")\n",
    "\n",
    "# antismash_data = get_gene_data(record)\n",
    "# for subunit in antismash_data:\n",
    "#     if len(subunit) == 6:\n",
    "#         print(subunit)\n",
    "\n",
    "subunits, standalones = process_cluster(record, os.path.join(file_path, entry[0] + '.json'))\n",
    "print(subunits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Process all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "helper = imp.load_source('helper', './pks/helper.py')\n",
    "domain = imp.load_source('domain', './pks/domain.py')\n",
    "pks = imp.load_source('pks', './pks/pks.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "entry = ('BGC0000166', 'U78289')\n",
    "record = SeqIO.read(os.path.join(antismash_file_path, entry[0] + '.embl'), \"embl\") \n",
    "#print(record.__dict__)\n",
    "print(record.annotations['comment'].split()[-1].strip().strip('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize dictionary to hold clusters and counter for valid clusters\n",
    "cluster_dict = {}\n",
    "valid = 0\n",
    "\n",
    "# Iterate over list of type I modular PKSs\n",
    "for i in range(len(t1pks)):\n",
    "    print('%d: %s' %(i, t1pks[i]))\n",
    "    entry = t1pks[i]\n",
    "\n",
    "     # This prints the accession number and product compound of the cluster\n",
    "    with open(os.path.join(file_path, entry[0] + '.json')) as json_file:\n",
    "        mibig_data = json.load(json_file)\n",
    "        pprint([compound['compound'] for compound in mibig_data['general_params']['compounds']])\n",
    "\n",
    "    # Read in cluster data\n",
    "    record = SeqIO.read(os.path.join(antismash_file_path, entry[0] + '.embl'), \"embl\")    \n",
    "\n",
    "    # Basic information\n",
    "    cluster_id = record.id\n",
    "    cluster_name = record.annotations['comment'].split()[-1].strip().strip('.')\n",
    "    cluster_description = record.description\n",
    "    cluster_sequence = record.seq\n",
    "    \n",
    "    # Subunit information\n",
    "    try:\n",
    "        subunits, standalones = process_cluster(record, os.path.join(file_path, entry[0] + '.json'))\n",
    "        cluster = pks.Cluster(cluster_id, cluster_name, cluster_description, cluster_sequence,\n",
    "                              subunits, standalones)\n",
    "        cluster_dict[cluster_id] = cluster\n",
    "        valid += 1\n",
    "    # If we can't process the cluster, then we show the exception\n",
    "    # We just use this for troubleshooting\n",
    "    except Exception as e:\n",
    "        print(Exception(type(e).__name__, e.args))\n",
    "\n",
    "# Print number of valid clusters\n",
    "print(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this to filter out clusters that don't seem to have any subunits\n",
    "npks_clusters = 0\n",
    "pruned_clusters = {}\n",
    "\n",
    "for key,value in cluster_dict.items():\n",
    "    cluster = value\n",
    "    subunits = cluster.subunits\n",
    "    if subunits and len(list(subunits.keys())) > 0:\n",
    "        print(cluster.description)\n",
    "#        print('\\t' + str(subunits))\n",
    "        pruned_clusters[key.split('.')[0]] = value\n",
    "        npks_clusters += 1\n",
    "\n",
    "print(npks_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "starter_units = []\n",
    "extender_units = []\n",
    "nmodules = 0\n",
    "for key,value in pruned_clusters.items():\n",
    "#    print key\n",
    "    cluster = value\n",
    "    print(cluster.description)\n",
    "    subunits = cluster.subunits\n",
    "    if len(subunits) == 0:\n",
    "        raise('Cluster has no subunits!')\n",
    "    for key,value in subunits.items():\n",
    "        subunit = value\n",
    "        print('\\t' + subunit.description)\n",
    "        for module in subunit.modules:\n",
    "            print('\\t' + str(list(module.domains.keys())))\n",
    "            print('\\t' + str(module.operations))\n",
    "            nmodules += 1\n",
    "            if module.loading == True:\n",
    "#                print '\\t' + module.domains['ATL'][1]['Substrate specificity predictions']\n",
    "                starter_units.append(module.domains['AT'][1]['Substrate specificity predictions'].split()[0])\n",
    "            else:\n",
    "#                print '\\t' + module.domains['AT'][1]['Substrate specificity predictions']\n",
    "                extender_units.append(module.domains['AT'][1]['Substrate specificity predictions'].split()[0])\n",
    "print(nmodules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(set(starter_units))\n",
    "print(set(extender_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Testing a single cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from rdkit.Chem.Draw import IPythonConsole"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "import imp\n",
    "# helper = imp.load_source('helper', './pks/helper.py')\n",
    "# domain = imp.load_source('domain', './pks/domain.py')\n",
    "# pks = imp.load_source('pks', './pks/pks.py')\n",
    "\n",
    "test_cluster_key = 'BGC0000165'\n",
    "record = SeqIO.read(os.path.join(antismash_file_path, test_cluster_key + '.embl'), \"embl\")\n",
    "subunits, standalones = process_cluster(record, os.path.join(file_path, test_cluster_key + '.json'))\n",
    "test_cluster = pks.Cluster('cluster_id', 'cluster_name', 'cluster_description', 'cluster_sequence',\n",
    "                           subunits, standalones)\n",
    "test_cluster.pop_subunit('tiaB')\n",
    "#test_cluster.update_subunit_order(['becB', 'becD', 'becE', 'becF', 'becG'])\n",
    "#test_cluster.toggle_cyclization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_cluster_key = 'BGC0001053'\n",
    "test_cluster = pruned_clusters[test_cluster_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(test_cluster_key)\n",
    "print(test_cluster.description)\n",
    "#print(list(test_cluster.subunits.values())[0].modules[0].operations[0].starter_name)\n",
    "subunits = list(test_cluster.subunits.values())\n",
    "chain = None\n",
    "for subunit in subunits:\n",
    "    print(subunit.name)\n",
    "    print(subunit.id)\n",
    "    for module in subunit.modules:\n",
    "        print(str(list(module.domains.keys())))\n",
    "        print(module.operations)\n",
    "    chain = subunit.compute_product(chain)\n",
    "    m_im = Draw.MolsToGridImage([chain], legends=[test_cluster.description], \n",
    "                                molsPerRow=1, subImgSize=(500,150), useSVG=True)\n",
    "    display(m_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Manually reorder clusters as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pruned_clusters['BGC0000097'].print_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# BE-14106\n",
    "pruned_clusters['BGC0000029'].update_subunit_order(['becB', 'becD', 'becE', 'becF', 'becG'])\n",
    "pruned_clusters['BGC0000029'].toggle_cyclization()\n",
    "# Cremimycin\n",
    "#pruned_clusters['BGC0000042'].update_subunit_order(['cmiP7', 'cmiP8', 'cmiP1', 'cmiP2', 'cmiP3', 'cmiP4', 'cmiP5', 'cmiP6'])\n",
    "# Meilingmycin\n",
    "pruned_clusters['BGC0000093'].pop_subunit('pks1')\n",
    "pruned_clusters['BGC0000093'].pop_subunit('pks2')\n",
    "# ML-449\n",
    "pruned_clusters['BGC0000097'].update_subunit_order(['mlaB', 'mlaD', 'mlaE', 'mlaF', 'mlaG'])\n",
    "pruned_clusters['BGC0000097'].toggle_cyclization()\n",
    "# Nystatin\n",
    "pruned_clusters['BGC0000115'].update_subunit_order(['nysA', 'nysB', 'nysC', 'nysI', 'nysJ', 'nysK'])\n",
    "# Tiacumicin\n",
    "pruned_clusters['BGC0000165'].pop_subunit('tiaB')\n",
    "# Simocyclinone\n",
    "pruned_clusters['BGC0001072'].update_subunit_order(['simC1A', 'simC1B', 'simC1C'])\n",
    "# Brasilinolide\n",
    "pruned_clusters['BGC0001381'].update_subunit_order(['nbrI', 'nbrJ', 'nbrK', 'nbrL', 'nbrG', 'nbrH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Testing all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(pruned_clusters, open('pruned_clusters_mibig.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid = 0\n",
    "nclusters = len(pruned_clusters)\n",
    "\n",
    "# Save invalid clusters so we can take a closer look\n",
    "pruned_clusters_invalid = {}\n",
    "\n",
    "for icluster,test_cluster_key in enumerate(list(pruned_clusters.keys())):\n",
    "    try:\n",
    "        test_cluster = pruned_clusters[test_cluster_key]\n",
    "        print('(%d/%d): %s' %(icluster, nclusters, test_cluster_key))\n",
    "        print(test_cluster.description)\n",
    "        print('\\tStarter from antiSMASH: %s' %(list(test_cluster.subunits.values())[0].modules[0].operations[0].starter_name))\n",
    "        with open(os.path.join(file_path, test_cluster_key + '.json')) as json_file:            \n",
    "            mibig_data = json.load(json_file)\n",
    "            try:\n",
    "                mibig_starter = mibig_data['general_params']['Polyketide']['starter_unit']\n",
    "            except KeyError:\n",
    "                mibig_starter = 'Not provided'\n",
    "        print('\\tStarter from MiBiG: %s' %(mibig_starter))        \n",
    "        subunits = list(test_cluster.subunits.values())\n",
    "        for subunit in subunits:\n",
    "            print(subunit.name)\n",
    "            for module in subunit.modules:\n",
    "                print('\\t' + str(list(module.domains.keys())))\n",
    "        m = test_cluster.compute_product(None)\n",
    "        m_im = Draw.MolsToGridImage([m], legends=[test_cluster.description], \n",
    "                                    molsPerRow=1, subImgSize=(500,150), useSVG=True)\n",
    "        display(m_im)\n",
    "        valid += 1\n",
    "    except Exception as e:\n",
    "        print('PREDICTION WARNING: %s' %(e))\n",
    "        \n",
    "        # Add to dictionary of invalid clusters\n",
    "        pruned_clusters_invalid[test_cluster_key] = test_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save invalid clusters so we can take a closer look\n",
    "pruned_clusters_invalid = {}\n",
    "\n",
    "for icluster,test_cluster_key in enumerate(list(pruned_clusters_invalid.keys())):\n",
    "    try:\n",
    "        test_cluster = pruned_clusters[test_cluster_key]\n",
    "        print('(%d/%d): %s' %(icluster, nclusters, test_cluster_key))\n",
    "        print('\\tStarter from antiSMASH: %s' %(list(test_cluster.subunits.values())[0].modules[0].operations[0].starter_name))\n",
    "        with open(os.path.join(file_path, test_cluster_key + '.json')) as json_file:            \n",
    "            mibig_data = json.load(json_file)\n",
    "            try:\n",
    "                mibig_starter = mibig_data['general_params']['Polyketide']['starter_unit']\n",
    "            except KeyError:\n",
    "                mibig_starter = 'Not provided'\n",
    "        print('\\tStarter from MiBiG: %s' %(mibig_starter))        \n",
    "        subunits = list(test_cluster.subunits.values())\n",
    "        for subunit in subunits:\n",
    "            print(subunit.name)\n",
    "            for module in subunit.modules:\n",
    "                print('\\t' + str(list(module.domains.keys())))\n",
    "        m = test_cluster.compute_product(None)\n",
    "        m_im = Draw.MolsToGridImage([m], legends=[test_cluster.description], \n",
    "                                    molsPerRow=1, subImgSize=(500,150), useSVG=True)\n",
    "        display(m_im)\n",
    "        valid += 1\n",
    "    except Exception as e:\n",
    "        print('WARNING: %s' %(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Printing subunit amino acid sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make sure all names are distinct\n",
    "subunit_names = []\n",
    "for icluster,test_cluster_key in enumerate(list(pruned_clusters.keys())):\n",
    "    test_cluster = pruned_clusters[test_cluster_key]\n",
    "    subunits = list(test_cluster.subunits.values())\n",
    "    for subunit in subunits:\n",
    "        subunit_names.append(test_cluster_key + subunit.name)\n",
    "print(len(subunit_names))\n",
    "print(len(set(subunit_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def write_fasta_for_cluster(cluster):\n",
    "    with open('./subunit_aa_sequences/' + str(cluster.name) + '.fasta', 'w') as f:\n",
    "        subunits = list(test_cluster.subunits.values())\n",
    "        for subunit in subunits:\n",
    "            sequence = subunit.sequence\n",
    "            # Write subunit to fasta file\n",
    "            f.write('>' + cluster.name + ',' + subunit.name + '\\n')\n",
    "            # In fasta format each line has only 80 characters\n",
    "            index = 0\n",
    "            written = 0\n",
    "            size = 80\n",
    "            while index < len(sequence) - 1:\n",
    "                try:\n",
    "                    segment = sequence[written*size:(written+1)*size]\n",
    "                except IndexError:\n",
    "                    segment = sequence[written*size:len(sequence - 1)]\n",
    "                f.write(segment + '\\n')\n",
    "                index += size\n",
    "                written += 1\n",
    "        \n",
    "nclusters = len(pruned_clusters)\n",
    "nsubunits = 0\n",
    "for icluster,test_cluster_key in enumerate(list(pruned_clusters.keys())):\n",
    "    test_cluster = pruned_clusters[test_cluster_key]\n",
    "    print('(%d/%d): %s' %(icluster, nclusters, test_cluster_key))\n",
    "    write_fasta_for_cluster(test_cluster)\n",
    "    nsubunits += len(test_cluster.subunits)\n",
    "print('Wrote out %d subunits from %d clusters.' %(nsubunits, nclusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
